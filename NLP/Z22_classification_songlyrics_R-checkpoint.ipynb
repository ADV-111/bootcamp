{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: the xgboost library is not installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost\n",
    "except ImportError as ex:\n",
    "    print(\"Error: the xgboost library is not installed.\")\n",
    "    xgboost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "to_remove= ('headers', 'footers', 'quotes')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='all', shuffle=True, remove = to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uwaga\n",
    "\n",
    "Zmniejszam zbiór danych biorąc okresloną ilość danych z każdej klasy\n",
    "\n",
    "5 klas po 500 elementów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for j in range(len(newsgroups_train.target_names)):\n",
    "    data.append([newsgroups_train.data[i] for i in range(newsgroups_train.target.shape[0]) if newsgroups_train.target[i] == j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biore okreslona ilosc danych z kazdej klasy\n",
    "\n",
    "categories = 5\n",
    "num=500\n",
    "X_set = []\n",
    "Y_set =[]\n",
    "for i in range(categories):\n",
    "    X_set = X_set + data[i][:num]\n",
    "    Y_set = Y_set + [i]*num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Znajdź najlepszy model dla 20newsgroups wykonując GridSearch dla modeli:\n",
    "\n",
    "* MultinomialNB (bez redukcji wymiarowości)\n",
    "* LogisticRegression\n",
    "* LinearSVC\n",
    "* SVC\n",
    "* KNeighborsClassifier\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* BaggingClassifier\n",
    "* ExtraTreesClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier\n",
    "* VotingClassifier\n",
    "* xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, Y_set, test_size=0.25, random_state=33)\n",
    "\n",
    "plt.hist(y_train, alpha=0.5)\n",
    "plt.hist(y_test, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    \n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)]\n",
    "}\n",
    "\n",
    "grid_0 = GridSearchCV(pipe, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_0.fit(X_train, y_train)\n",
    "grid_0.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=2)), \n",
    "    ('classifier', LinearSVC(C=1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "            'classifier__C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', SVC(C=1, probability=True))\n",
    "])\n",
    "\n",
    "param_grid_2 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100,],\n",
    "            'classifier__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_2 = GridSearchCV(pipe_2, param_grid_2, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_2.fit(X_train, y_train)\n",
    "grid_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', LogisticRegression(C=1))\n",
    "])\n",
    "\n",
    "param_grid_3 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_3 = GridSearchCV(pipe_3, param_grid_3, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_3.fit(X_train, y_train)\n",
    "grid_3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe_4 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=2, metric='euclidean'))\n",
    "])\n",
    "\n",
    "param_grid_4 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__n_neighbors': [2, 5, 10, 11,12],\n",
    "            'classifier__metric': ['euclidean', 'cityblock', 'cosine']\n",
    "}\n",
    "\n",
    "\n",
    "grid_4 = GridSearchCV(pipe_4, param_grid_4, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_4.fit(X_train, y_train)\n",
    "grid_4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_5 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "param_grid_5 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__max_depth': [5,9,10,11,20,30],\n",
    "            'classifier__min_samples_split': [2,3,5,10,20,30,40],\n",
    "            'classifier__max_leaf_nodes': [3,4,10,14,15,16,20,30,40]\n",
    "}\n",
    "\n",
    "\n",
    "grid_5 = GridSearchCV(pipe_5, param_grid_5, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_5.(X_train, y_train)\n",
    "grid_5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_6 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', BaggingClassifier(\n",
    "                                    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "                                    max_samples=100, bootstrap=True, random_state=42))\n",
    "                  ])\n",
    "\n",
    "param_grid_6 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__n_estimators': [10,50,100],\n",
    "            'classifier__max_samples': [10,20]\n",
    "             }\n",
    "\n",
    "grid_6 = GridSearchCV(pipe_6, param_grid_6, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_6.fit(X_train, y_train)\n",
    "grid_6.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe_7 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', RandomForestClassifier(n_estimators=500, max_leaf_nodes=16))\n",
    "                  ])\n",
    "\n",
    "param_grid_7 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__n_estimators': [10, 50, 100],\n",
    "            'classifier__max_leaf_nodes': [10, 20],\n",
    "            'classifier__max_depth': [10, 20]\n",
    "             }\n",
    "\n",
    "grid_7 = GridSearchCV(pipe_7, param_grid_7, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_7.fit(X_train, y_train)\n",
    "grid_7.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "pipe_8 = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16))\n",
    "                  ])\n",
    "\n",
    "param_grid_8 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [10, 50, 100],\n",
    "                'classifier__max_leaf_nodes': [10, 20],\n",
    "                'classifier__max_depth': [10, 20]\n",
    "             }\n",
    "\n",
    "grid_8 = GridSearchCV(pipe_8, param_grid_8, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_8.fit(X_train, y_train)\n",
    "grid_8.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_9 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', AdaBoostClassifier(\n",
    "                        DecisionTreeClassifier(max_depth=1), \n",
    "                        n_estimators=1, learning_rate=0.5, \n",
    "                        algorithm=\"SAMME.R\", random_state=42)\n",
    "                   )\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_9 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_9 = GridSearchCV(pipe_9, param_grid_9, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_9.fit(X_train, y_train)\n",
    "grid_9.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipe_10 = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', GradientBoostingClassifier(\n",
    "                       n_estimators=1, \n",
    "                      learning_rate=0.5, \n",
    "                      random_state=42))\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_10 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_10 = GridSearchCV(pipe_10, param_grid_10, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_10.fit(X_train, y_train)\n",
    "grid_10.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_11 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', xgboost.XGBClassifier(n_estimators=1, \n",
    "                      learning_rate=0.5, \n",
    "                      random_state=42))\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_11 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_11 = GridSearchCV(pipe_11, param_grid_11, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_11.fit(X_train, y_train)\n",
    "grid_11.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('grid_2', grid_2.best_estimator_), \n",
    "                ('grid_3', grid_3.best_estimator_), \n",
    "                ('grid_4', grid_4.best_estimator_), \n",
    "                ('grid_5', grid_5.best_estimator_), \n",
    "                ('grid_6', grid_6.best_estimator_), \n",
    "                ('grid_7', grid_7.best_estimator_), \n",
    "                ('grid_8', grid_8.best_estimator_), \n",
    "                ('grid_9', grid_9.best_estimator_),\n",
    "                ('grid_10', grid_10.best_estimator_), \n",
    "                ('grid_11', grid_11.best_estimator_)\n",
    "               ],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "estimator = xgboost.XGBClassifier(n_jobs=-1)\n",
    "\n",
    "pipe_12 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', xgboost.XGBClassifier(n_jobs=-1))\n",
    "                  ])\n",
    "\n",
    "param_grid_12 = {\n",
    "    'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                        stop_words = 'english',\n",
    "                        lowercase = True,\n",
    "                        token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                        max_df = 0.5, \n",
    "                        min_df = 10), \n",
    "                   CountVectorizer(strip_accents = 'unicode',\n",
    "                        stop_words = 'english',\n",
    "                        lowercase = True,\n",
    "                        token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                        max_df = 0.5, \n",
    "                        min_df = 10)],\n",
    "    'classifier__max_depth': [3, 5, 8, 10],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'classifier__n_estimators': [50, 100, 150, 200, 400],\n",
    "    'classifier__gamma': [0, 0.5, 1, 2],\n",
    "    'classifier__colsample_bytree': [1, 0.8, 0.5],\n",
    "    'classifier__subsample': [1, 0.8, 0.5],\n",
    "    'classifier__min_child_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_12 = RandomizedSearchCV(n_iter=30,estimator=pipe_12, \n",
    "                             param_distributions=param_grid_12, \n",
    "                      cv=kfold, \n",
    "                      return_train_score=True)\n",
    "\n",
    "grid_12.fit(X_train, y_train)\n",
    "grid_12.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('NB', grid_0.best_estimator_))\n",
    "models.append(('SVM linear', grid_1.best_estimator_))\n",
    "# models.append(('SVM rbf', grid_2.best_estimator_))\n",
    "# models.append(('LR', grid_3.best_estimator_))\n",
    "# models.append(('KNN', grid_4.best_estimator_))\n",
    "# models.append(('DecisionTreeClassifier', grid_5.best_estimator_))\n",
    "# models.append(('BaggingClassifier', grid_6.best_estimator_))\n",
    "# models.append(('RandomForestClassifier', grid_7.best_estimator_))\n",
    "# models.append(('ExtraTreesClassifier', grid_8.best_estimator_))\n",
    "# models.append(('AdaBoostClassifier', grid_9.best_estimator_))\n",
    "# models.append(('GradientBoostingClassifier', grid_10.best_estimator_))\n",
    "# models.append(('XGBClassifier', grid_11.best_estimator_))\n",
    "# models.append(('voting_clf', voting_clf))\n",
    "# models.append(('XGBClassifier r2', grid_12.best_estimator_))\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "roc_auc_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test , model.predict(X_test)) ))\n",
    "    \n",
    "\n",
    "    precision_score.append(metrics.precision_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    recall_score.append(metrics.recall_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    f1_score.append( metrics.f1_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test , model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['NB', 'SVM linear'])\n",
    "# df.insert(loc=0, column='Method', value=['NB', 'SVM linear','SVM rbf','LR','KNN','DecisionTreeClassifier','BaggingClassifier','RandomForestClassifier','ExtraTreesClassifier', 'AdaBoostClassifier','GradientBoostingClassifier','XGBClassifier','voting','XGBClassifier r'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
