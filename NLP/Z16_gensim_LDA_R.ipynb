{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "LSI pozwala w pewnym sensie znajdowac kluczowe tematy w tekście i dla danego dokumentu określać najbliższy mu temat. Jest to podejście geometryczne. \n",
    "\n",
    "LDA jest podejściem probabilistycznym do modelowania tematów. Jest bardziej dokładny choć wolniejszy.\n",
    "\n",
    "# Model LDA - Latent Dirichlet Allocation (ukryta alokacja Dirichleta)\n",
    "\n",
    "\n",
    "Motywacja: przedstawienie tekstu jako mieszanki tematów.\n",
    "\n",
    "\n",
    "Temat - rozkład prawdopodobieństwa na zbiorze słów.\n",
    "\n",
    "\n",
    "Przykład:\n",
    "*  <s>Mam</s> gorączkę <s>i</s> katar.\n",
    "* Graliśmy <s>w</s> siatkówkę.\n",
    "* Grając <s>w</s> piłkę, wzmacniamy organizm.\n",
    "\n",
    "\n",
    "Ile \"tematów\" widzimy?\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Intuicyjnie: dwa tematy: \"sport\" oraz \"zdrowie\".\n",
    "* Pierwsze zdanie = 100% zdrowie\n",
    "* Drugie zdanie = 100% sport\n",
    "* Trzecie zdanie = 50% sport + 50% zdrowie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA to model probabilistyczny, który wykorzystuje dwie wartości prawdopodobieństwa: \n",
    "\n",
    "* P (słowo | tematy) \n",
    "* P (tematy | dokumenty) \n",
    "\n",
    "do stworzenia klastrów.\n",
    "\n",
    "\n",
    "Proces budowy modelu LDA zakłada więc:\n",
    "* identyfikację tematów reprezentowanych przez dokumenty korpusu,\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia wyrazów dla każdego z tematów (stosowany jest rozkład Dirichleta),\n",
    "* oszacowanie rozkładu prawdopodobieństw wystąpienia tematów w rozpatrywanych dokumentach (stosowany jest również rozkład Dirichleta).\n",
    "\n",
    "Wykorzystując metodę identyfikacji słów kluczowych opartą na ukrytej alokacji Dirichleta można:\n",
    "* dla każdego dokumentu obliczyć prawdopodobieństwo wystąpienia w nim każdego z tematów;\n",
    "* dla każdego tematu obliczyć prawdopodobieństwo wystąpienia słów;\n",
    "* obliczyć prawdopodobieństwa wystąpienia poszczególnych słów w dokumencie (jako iloczyny dwóch wyżej wymienionych prawdopodobieństw) \n",
    "\n",
    "# Generatywność\n",
    "\n",
    "Mając prawdopodobieństwa $p_1 = P (słowo | tematy) $, $p_2 = P (tematy | dokumenty)$ możemy wygeneraować dokument:\n",
    "\n",
    "- wybieramy (losujemy) prawdopodobieństwo przynależności dokumentu do tematu (z $p_1$) - dokuemnt zawsze należy do wielu tematów\n",
    "\n",
    "- generujemy słowa w dokumencie - najpierw losujemy do jakiego tematu należy słowo, a potem generujemy słowo z tego tematu (z $p_2$)\n",
    "\n",
    "# Uczenie\n",
    "\n",
    "1. Idziemy przez wszystkie słowa i wszystkie dokumentu i losowo przyporządkowujemy je do tematów.\n",
    "2. Iteracyjnie idzemy: bierzemy dokuemnt $d$  i słowo $w$ i przyporządkowujemy je do najlepiej pasującego tematu - maksymalizującego $P (słowo | tematy) \n",
    "* P (tematy | dokumenty) $\n",
    "3. Po przejsciu przez wszystkie dokumenty aktualizujemy przydział słów do tematów i tematów do dokumentów i wracamy do punktu 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Na początek wczytujemy korpus z dysku. Użyjemy przykład stworzonego w poprzednim notebook-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [\"Romeo and Juliet\",\n",
    "#          \"Juliet: O happy dagger\",\n",
    "#          \"Romeo died by dagger\",\n",
    "#          \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "#          \"Did you know, New-Hampshire is in New-England\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(\"tmp/deerwester.dict\")):\n",
    "    dictionary = corpora.Dictionary.load('tmp/deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('tmp/deerwester.mm')\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "for d in corpus:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "scipy_csc_matrix = matutils.corpus2csc(corpus)\n",
    "print(scipy_csc_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'juliet': 0, 'romeo': 1, 'dagger': 2, 'happy': 3, 'juliet:': 4, 'o': 5, 'by': 6, 'died': 7, \"'live\": 8, \"die',\": 9, 'free': 10, 'motto': 11, 'new-hampshire’s': 12, 'or': 13, 'that’s': 14, 'did': 15, 'is': 16, 'know,': 17, 'new-england': 18, 'new-hampshire': 19, 'you': 20}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budujemy model LDA\n",
    "Budujemy model LDA i transformujemy dane\n",
    "\n",
    "* **num_topics=2** oznacza ilość modelowanych tematów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7573557), (1, 0.24264431)]\n",
      "[(0, 0.12710506), (1, 0.87289494)]\n",
      "[(0, 0.8710887), (1, 0.12891136)]\n",
      "[(0, 0.07185894), (1, 0.92814106)]\n",
      "[(0, 0.89790803), (1, 0.10209194)]\n"
     ]
    }
   ],
   "source": [
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "corpus_lda = model[corpus] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "for d in corpus_lda:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dla każdego dokumentu dostajemy prawdopodobieństwo przynależności dokumentu do danego tematu.\n",
    "\n",
    "Możemy też zobaczyć z czego składają się tematy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.086*\"romeo\" + 0.067*\"dagger\" + 0.067*\"died\" + 0.063*\"by\" + 0.058*\"new-england\" + 0.054*\"did\" + 0.054*\"new-hampshire\" + 0.053*\"know,\" + 0.052*\"you\" + 0.050*\"is\"'),\n",
       " (1,\n",
       "  '0.069*\"dagger\" + 0.057*\"or\" + 0.056*\"die\\',\" + 0.055*\"happy\" + 0.055*\"that’s\" + 0.054*\"\\'live\" + 0.054*\"romeo\" + 0.054*\"motto\" + 0.053*\"new-hampshire’s\" + 0.053*\"free\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.086*\"romeo\" + 0.067*\"dagger\" + 0.067*\"died\" + 0.063*\"by\" + 0.058*\"new-england\" + 0.054*\"did\" + 0.054*\"new-hampshire\" + 0.053*\"know,\" + 0.052*\"you\" + 0.050*\"is\"'),\n",
       " (1,\n",
       "  '0.069*\"dagger\" + 0.057*\"or\" + 0.056*\"die\\',\" + 0.055*\"happy\" + 0.055*\"that’s\" + 0.054*\"\\'live\" + 0.054*\"romeo\" + 0.054*\"motto\" + 0.053*\"new-hampshire’s\" + 0.053*\"free\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.086*\"romeo\" + 0.067*\"dagger\" + 0.067*\"died\" + 0.063*\"by\"'),\n",
       " (1, '0.069*\"dagger\" + 0.057*\"or\" + 0.056*\"die\\',\" + 0.055*\"happy\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_topics=2, num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Chcemy posortować słowa każdego tematu i wybrać 5 najważniejszych - co można powiedzieć o tematach?\n",
    "\n",
    "Proszę zobaczyć na funkcje typu get_topics(), get_term_topics(...): https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeo\n",
      "dagger\n",
      "died\n",
      "by\n",
      "new-england\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topics = np.argsort(model.get_topics()[0,:])[::-1] #::-1 sortowanie w odwrotnej kolejności\n",
    "for x in topics[:5]:\n",
    "    print(dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.086217955),\n",
       " (2, 0.06702697),\n",
       " (7, 0.06671357),\n",
       " (6, 0.062788315),\n",
       " (18, 0.05837199),\n",
       " (15, 0.054136433),\n",
       " (19, 0.053630922),\n",
       " (17, 0.05346427),\n",
       " (20, 0.051601455),\n",
       " (16, 0.049574506)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_terms(topicid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romeo', 0.086217955),\n",
       " ('dagger', 0.06702697),\n",
       " ('died', 0.06671357),\n",
       " ('by', 0.062788315),\n",
       " ('new-england', 0.05837199),\n",
       " ('did', 0.054136433),\n",
       " ('new-hampshire', 0.053630922),\n",
       " ('know,', 0.05346427),\n",
       " ('you', 0.051601455),\n",
       " ('is', 0.049574506)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic2_terms = model.get_topic_terms(topicid=0)\n",
    "topic2_words = [\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in topic2_terms\n",
    "]\n",
    "topic2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romeo', 0.086217955),\n",
       " ('dagger', 0.06702697),\n",
       " ('died', 0.06671357),\n",
       " ('by', 0.062788315),\n",
       " ('new-england', 0.05837199),\n",
       " ('did', 0.054136433),\n",
       " ('new-hampshire', 0.053630922),\n",
       " ('know,', 0.05346427),\n",
       " ('you', 0.051601455),\n",
       " ('is', 0.049574506)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (dictionary.get(i), j)\n",
    "    for i,j in model.get_topic_terms(topicid=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "\n",
    "Proszę posortować zdania najbardziej pasujące do danego tematu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    \n",
    "#trzeba by wypisać raczej zdania niż ich reprezentacje bag-of-words, ale tu nie mam dostepu do tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0)]\n",
      "[(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(0, 1.0), (1, 1.0)]\n",
      "[(1, 1.0), (2, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(corpus[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know, New-Hampshire is in New-England\n",
      "Romeo died by dagger\n",
      "Romeo and Juliet\n",
      "Juliet: O happy dagger\n",
      "'Live free or die', that’s the New-Hampshire’s motto\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Romeo and Juliet\",\n",
    "         \"Juliet: O happy dagger\",\n",
    "         \"Romeo died by dagger\",\n",
    "         \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "         \"Did you know, New-Hampshire is in New-England\"]\n",
    "\n",
    "top_inex = 0\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort( numpy_corpus[top_inex,:] )[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Live free or die', that’s the New-Hampshire’s motto\n",
      "Juliet: O happy dagger\n",
      "Romeo and Juliet\n",
      "Romeo died by dagger\n",
      "Did you know, New-Hampshire is in New-England\n"
     ]
    }
   ],
   "source": [
    "top_inex = 1\n",
    "\n",
    "numpy_corpus = gensim.matutils.corpus2dense(corpus_lda, num_terms=2)\n",
    "docs = np.argsort(numpy_corpus[top_inex,:])[::-1]\n",
    "for x in docs[:5]:\n",
    "    print(documents[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "Sprawdzić do jakiego tematu pasuje nowy dokument i jakie są mu najbliższe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"died dagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7674487), (1, 0.23255134)]\n"
     ]
    }
   ],
   "source": [
    "doc_rep = dictionary.doc2bow(doc.split(' '))\n",
    "# print(doc_rep)\n",
    "doc_assignments = model[doc_rep]\n",
    "print(doc_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9998754), (1, 0.4249646), (2, 0.98917943), (3, 0.3630086), (4, 0.9836546)]\n",
      "[(0, 0.9998754), (2, 0.98917943), (4, 0.9836546), (1, 0.4249646), (3, 0.3630086)]\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(corpus_lda)\n",
    "\n",
    "sims = index[doc_assignments]\n",
    "print(list(enumerate(sims)))\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wizualizacja modelu LDA:\n",
    "\n",
    "pyLDAvis\n",
    "\n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "\n",
    "* Czerwone słupki reprezentują częstotliwość słów w danym temacie (proporcjonalnie do $P (słowo \\ | \\ tematy) $), \n",
    "* Niebieskie słupki reprezentują częstotliwość tematów w całym korpusie (proporcjonalnie do $P(tematy \\ | \\ dokumenty)$. \n",
    "\n",
    "Zmień wartość $\\lambda$, aby dostosować rankingi słów: \n",
    " * małe wartości $\\lambda$ (blisko 0) podkreślają potencjalnie rzadkie, ale ekskluzywne słowa dla wybranego tematu\n",
    " * duże wartości $\\lambda$ (blisko 1) podkreślają częste, ale niekoniecznie ekskluzywne słowa dla wybranego tematu. \n",
    " \n",
    "W http://www.kennyshirley.com/LDAvis/ sugeruje się żeby ustawiać $\\lambda$ w pobliżu 0.6. Podobno pomaga to użytkownikom w interpretacji tematu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.gensim.prepare??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\przem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el144814968151397204436484394\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el144814968151397204436484394_data = {\"mdsDat\": {\"x\": [0.011679184623062609, -0.011679184623062609], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [50.44253921508789, 49.557456970214844]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6607159376144409, 0.6455578804016113, 0.6380960941314697, 0.6376944780349731, 0.6277443766593933, 0.6218351125717163, 0.619716465473175, 0.6152997612953186, 0.6118327975273132, 0.5883269309997559, 0.8016972541809082, 0.4903543293476105, 0.4892979562282562, 0.4705743193626404, 0.45336756110191345, 0.45182788372039795, 0.4471590518951416, 0.6244279742240906, 0.4080342650413513, 0.367240309715271, 0.3309829533100128, 0.7604156732559204, 0.7156747579574585, 0.665336549282074, 0.9827308058738708, 0.6170586943626404, 0.6112967729568481, 0.609397292137146, 0.5881645083427429, 0.5650609135627747, 0.5637568235397339, 0.7639878392219543, 0.4428618252277374, 0.4138566553592682, 0.4095779061317444, 0.40412893891334534, 0.4015141427516937, 0.3942226469516754, 0.3819445073604584, 0.3814490735530853, 0.3722412586212158, 0.3535371422767639], \"Term\": [\"died\", \"by\", \"romeo\", \"or\", \"new-england\", \"die',\", \"happy\", \"that\\u2019s\", \"'live\", \"motto\", \"new-hampshire\\u2019s\", \"free\", \"did\", \"juliet:\", \"new-hampshire\", \"know,\", \"you\", \"o\", \"is\", \"juliet\", \"dagger\", \"or\", \"die',\", \"happy\", \"that\\u2019s\", \"'live\", \"motto\", \"new-hampshire\\u2019s\", \"free\", \"juliet:\", \"o\", \"dagger\", \"juliet\", \"is\", \"you\", \"know,\", \"new-hampshire\", \"did\", \"romeo\", \"new-england\", \"by\", \"died\", \"died\", \"by\", \"new-england\", \"romeo\", \"did\", \"new-hampshire\", \"know,\", \"you\", \"is\", \"juliet\", \"dagger\", \"o\", \"juliet:\", \"free\", \"new-hampshire\\u2019s\", \"motto\", \"'live\", \"that\\u2019s\", \"happy\", \"die',\", \"or\"], \"Total\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0142531394958496, 1.0177991390228271, 1.0195451974868774, 1.019639015197754, 1.0219670534133911, 1.0233492851257324, 1.0238454341888428, 1.024877667427063, 1.0256894826889038, 1.031188726425171, 1.5656850337982178, 1.054111123085022, 1.0543588399887085, 1.0587388277053833, 1.0627648830413818, 1.063124656677246, 1.0642178058624268, 1.6071587800979614, 1.0733708143234253, 1.0829150676727295, 1.0913985967636108, 1.0913985967636108, 1.0829150676727295, 1.0733708143234253, 1.6071587800979614, 1.0642178058624268, 1.063124656677246, 1.0627648830413818, 1.0587388277053833, 1.0543588399887085, 1.054111123085022, 1.5656850337982178, 1.031188726425171, 1.0256894826889038, 1.024877667427063, 1.0238454341888428, 1.0233492851257324, 1.0219670534133911, 1.019639015197754, 1.0195451974868774, 1.0177991390228271, 1.0142531394958496], \"loglift\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.2558000087738037, 0.22910000383853912, 0.21570000052452087, 0.2150000035762787, 0.19699999690055847, 0.18619999289512634, 0.18230000138282776, 0.17409999668598175, 0.16769999265670776, 0.12319999933242798, 0.014999999664723873, -0.08100000023841858, -0.08340000361204147, -0.1264999955892563, -0.16760000586509705, -0.1712999939918518, -0.1826999932527542, -0.26109999418258667, -0.28290000557899475, -0.3971000015735626, -0.5088000297546387, 0.3407000005245209, 0.28790000081062317, 0.22380000352859497, 0.210099995136261, 0.15700000524520874, 0.14869999885559082, 0.14589999616146088, 0.11420000344514847, 0.07829999923706055, 0.07620000094175339, -0.01549999974668026, -0.14319999516010284, -0.20559999346733093, -0.2152000069618225, -0.22750000655651093, -0.23360000550746918, -0.25049999356269836, -0.2799000144004822, -0.28110000491142273, -0.30379998683929443, -0.35190001130104065], \"logprob\": [21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.865600109100342, -2.8887999057769775, -2.900399923324585, -2.901099920272827, -2.916800022125244, -2.9261999130249023, -2.9296998977661133, -2.936800003051758, -2.942500114440918, -2.981600046157837, -2.6721999645233154, -3.163800001144409, -3.1658999919891357, -3.2049999237060547, -3.2421998977661133, -3.2455999851226807, -3.25600004196167, -2.922100067138672, -3.347599983215332, -3.452899932861328, -3.55679988861084, -2.7072999477386475, -2.7679998874664307, -2.84089994430542, -2.450900077819824, -2.9161999225616455, -2.925600051879883, -2.9286999702453613, -2.964200019836426, -3.004300117492676, -3.0065999031066895, -2.702699899673462, -3.247999906539917, -3.315700054168701, -3.3261001110076904, -3.3394999504089355, -3.3459999561309814, -3.364300012588501, -3.395900011062622, -3.397200107574463, -3.4217000007629395, -3.4732000827789307]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2], \"Freq\": [0.9785051345825195, 0.9234334230422974, 0.6386980414390564, 0.6386980414390564, 0.9396572709083557, 0.9825121164321899, 0.916255533695221, 0.9757261872291565, 0.9808294773101807, 0.9484437108039856, 0.9486665725708008, 0.9749539494514465, 0.9409418702125549, 0.97718346118927, 0.9316444993019104, 0.9406234622001648, 0.9767099022865295, 0.9697545766830444, 0.9859471321105957, 0.6222160458564758, 0.6222160458564758, 0.9807392358779907, 0.9445199966430664], \"Term\": [\"'live\", \"by\", \"dagger\", \"dagger\", \"did\", \"die',\", \"died\", \"free\", \"happy\", \"is\", \"juliet\", \"juliet:\", \"know,\", \"motto\", \"new-england\", \"new-hampshire\", \"new-hampshire\\u2019s\", \"o\", \"or\", \"romeo\", \"romeo\", \"that\\u2019s\", \"you\"]}, \"R\": 21, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el144814968151397204436484394\", ldavis_el144814968151397204436484394_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el144814968151397204436484394\", ldavis_el144814968151397204436484394_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el144814968151397204436484394\", ldavis_el144814968151397204436484394_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "1      0.011679  0.0       1        1  50.442539\n",
       "0     -0.011679  0.0       2        1  49.557457, topic_info=     Category      Freq             Term     Total  loglift  logprob\n",
       "term                                                                \n",
       "7     Default  1.000000             died  1.000000  21.0000  21.0000\n",
       "6     Default  1.000000               by  1.000000  20.0000  20.0000\n",
       "1     Default  1.000000            romeo  1.000000  19.0000  19.0000\n",
       "13    Default  1.000000               or  1.000000  18.0000  18.0000\n",
       "18    Default  1.000000      new-england  1.000000  17.0000  17.0000\n",
       "9     Default  1.000000            die',  1.000000  16.0000  16.0000\n",
       "3     Default  1.000000            happy  1.000000  15.0000  15.0000\n",
       "14    Default  1.000000           that’s  1.000000  14.0000  14.0000\n",
       "8     Default  1.000000            'live  1.000000  13.0000  13.0000\n",
       "11    Default  1.000000            motto  1.000000  12.0000  12.0000\n",
       "12    Default  1.000000  new-hampshire’s  1.000000  11.0000  11.0000\n",
       "10    Default  1.000000             free  1.000000  10.0000  10.0000\n",
       "15    Default  1.000000              did  1.000000   9.0000   9.0000\n",
       "4     Default  1.000000          juliet:  1.000000   8.0000   8.0000\n",
       "19    Default  1.000000    new-hampshire  1.000000   7.0000   7.0000\n",
       "17    Default  1.000000            know,  1.000000   6.0000   6.0000\n",
       "20    Default  1.000000              you  1.000000   5.0000   5.0000\n",
       "5     Default  1.000000                o  1.000000   4.0000   4.0000\n",
       "16    Default  1.000000               is  1.000000   3.0000   3.0000\n",
       "0     Default  1.000000           juliet  1.000000   2.0000   2.0000\n",
       "2     Default  1.000000           dagger  1.000000   1.0000   1.0000\n",
       "13     Topic1  0.660716               or  1.014253   0.2558  -2.8656\n",
       "9      Topic1  0.645558            die',  1.017799   0.2291  -2.8888\n",
       "3      Topic1  0.638096            happy  1.019545   0.2157  -2.9004\n",
       "14     Topic1  0.637694           that’s  1.019639   0.2150  -2.9011\n",
       "8      Topic1  0.627744            'live  1.021967   0.1970  -2.9168\n",
       "11     Topic1  0.621835            motto  1.023349   0.1862  -2.9262\n",
       "12     Topic1  0.619716  new-hampshire’s  1.023845   0.1823  -2.9297\n",
       "10     Topic1  0.615300             free  1.024878   0.1741  -2.9368\n",
       "4      Topic1  0.611833          juliet:  1.025689   0.1677  -2.9425\n",
       "...       ...       ...              ...       ...      ...      ...\n",
       "16     Topic1  0.489298               is  1.054359  -0.0834  -3.1659\n",
       "20     Topic1  0.470574              you  1.058739  -0.1265  -3.2050\n",
       "17     Topic1  0.453368            know,  1.062765  -0.1676  -3.2422\n",
       "19     Topic1  0.451828    new-hampshire  1.063125  -0.1713  -3.2456\n",
       "15     Topic1  0.447159              did  1.064218  -0.1827  -3.2560\n",
       "1      Topic1  0.624428            romeo  1.607159  -0.2611  -2.9221\n",
       "18     Topic1  0.408034      new-england  1.073371  -0.2829  -3.3476\n",
       "6      Topic1  0.367240               by  1.082915  -0.3971  -3.4529\n",
       "7      Topic1  0.330983             died  1.091399  -0.5088  -3.5568\n",
       "7      Topic2  0.760416             died  1.091399   0.3407  -2.7073\n",
       "6      Topic2  0.715675               by  1.082915   0.2879  -2.7680\n",
       "18     Topic2  0.665337      new-england  1.073371   0.2238  -2.8409\n",
       "1      Topic2  0.982731            romeo  1.607159   0.2101  -2.4509\n",
       "15     Topic2  0.617059              did  1.064218   0.1570  -2.9162\n",
       "19     Topic2  0.611297    new-hampshire  1.063125   0.1487  -2.9256\n",
       "17     Topic2  0.609397            know,  1.062765   0.1459  -2.9287\n",
       "20     Topic2  0.588165              you  1.058739   0.1142  -2.9642\n",
       "16     Topic2  0.565061               is  1.054359   0.0783  -3.0043\n",
       "0      Topic2  0.563757           juliet  1.054111   0.0762  -3.0066\n",
       "2      Topic2  0.763988           dagger  1.565685  -0.0155  -2.7027\n",
       "5      Topic2  0.442862                o  1.031189  -0.1432  -3.2480\n",
       "4      Topic2  0.413857          juliet:  1.025689  -0.2056  -3.3157\n",
       "10     Topic2  0.409578             free  1.024878  -0.2152  -3.3261\n",
       "12     Topic2  0.404129  new-hampshire’s  1.023845  -0.2275  -3.3395\n",
       "11     Topic2  0.401514            motto  1.023349  -0.2336  -3.3460\n",
       "8      Topic2  0.394223            'live  1.021967  -0.2505  -3.3643\n",
       "14     Topic2  0.381945           that’s  1.019639  -0.2799  -3.3959\n",
       "3      Topic2  0.381449            happy  1.019545  -0.2811  -3.3972\n",
       "9      Topic2  0.372241            die',  1.017799  -0.3038  -3.4217\n",
       "13     Topic2  0.353537               or  1.014253  -0.3519  -3.4732\n",
       "\n",
       "[63 rows x 6 columns], token_table=      Topic      Freq             Term\n",
       "term                                  \n",
       "8         1  0.978505            'live\n",
       "6         2  0.923433               by\n",
       "2         1  0.638698           dagger\n",
       "2         2  0.638698           dagger\n",
       "15        2  0.939657              did\n",
       "9         1  0.982512            die',\n",
       "7         2  0.916256             died\n",
       "10        1  0.975726             free\n",
       "3         1  0.980829            happy\n",
       "16        2  0.948444               is\n",
       "0         2  0.948667           juliet\n",
       "4         1  0.974954          juliet:\n",
       "17        2  0.940942            know,\n",
       "11        1  0.977183            motto\n",
       "18        2  0.931644      new-england\n",
       "19        2  0.940623    new-hampshire\n",
       "12        1  0.976710  new-hampshire’s\n",
       "5         1  0.969755                o\n",
       "13        1  0.985947               or\n",
       "1         1  0.622216            romeo\n",
       "1         2  0.622216            romeo\n",
       "14        1  0.980739           that’s\n",
       "20        2  0.944520              you, R=21, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
